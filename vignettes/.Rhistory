smooth.tmp=get.smooth.o(bic.result[[i]]$solution, TB.o, time)
output[[OL+2*i-1]] <- smooth.tmp
names(output)[OL+2*i-1] <- paste0("TB.o.smooth.",penalty[i])
output[[OL+2*i]] <- get.surv(smooth.tmp)
names(output)[OL+2*i] <- paste0("TB.s.smooth.",penalty[i])
}
#chosen bandwidth
OL <- length(output)
for(i in 1:length(bic.result)){
output[[OL+i]] <- bic.result[[i]]$solution
names(output)[OL+i] <- paste0(penalty[i],"_bw")
}
output[[length(output)+1]] <- dat
names(output)[length(output)] <- "dat"
return(output)
}
#' Nonparametric Estimation of a Smoothed Kaplan-Meier Survival Curve
#'
#' This function estimates survival curves (and time-to-event curves) from interval censored data
#' using the method of Kaplan & Meier (1958) and subsequently finds an optimal smoothing bandwidth which
#' minimizes the a penalized log-likelihood function (sBIC) as described in our manuscript.
#'
#' The function takes a matrix or data frame as input, where each row represents a subject.
#' The first column should be either the time at event or the time at last follow up (if the
#' subject is right-censored). The second column is a binary variable indicating whether the subject
#' was observed to experience an event (1) or not (0).
#'
#' The output is a list containing the original and smoothed Kaplan-Meier survival and time-to-event
#' distributions among other sample and algorithm characteristics.
#'
#' @param dat A data.frame or matrix where rows are subjects and columns are the event/censoring time and event indicator.
#' @param n.obs The number of observations per subject. Used for calculation of effective N. Defaults to 2.
#' @param left.bound The earliest possible time which an event can occur. Defaults to 0.
#' @param penalty The penalty/penalties to use when calculating the sBIC. Possible values are "logNe",
#' "logNm", or "logN".
#' @param n.dec The number of decimal places in the observed data.
#' @param tolerance The tolerance for change in bandwidth when performing local optimization of the sBIC.
#' @param inflection.threshold Threshold used when counting the number of turning points in the time
#' to event density curve. Note that deviations from the default value have not been extensively tested.
#'
#' @export
smoothKM <- function(dat,
n.obs = 2,
left.bound = 0,
penalty = c("logN","logNm","logNe"),
n.dec = 2,
tolerance = NA,
inflection.threshold = 1e-2) {
#BIC constant
N <- nrow(dat)
n.dec <- n.dec
time.int <- (1/(10^(n.dec)))
#vector of times under consideration
time <- round(seq(from = left.bound, to = max(dat[,1:2], na.rm = T), by = time.int), n.dec)
#round the data to n.dec
dat[,1] <- round(dat[,1], n.dec)
#apply kaplan-meier
survfit.mod <- survival::survfit(survival::Surv(time = dat[,1], event = dat[,2]) ~ 1)
#get survival
#survival::summary.survfit
KM.s <- summary(survfit.mod,time = time)$surv
KM.s[(length(KM.s)+1):length(time)] <- KM.s[length(KM.s)]
#get onset
KM.o <- diff(1-KM.s)
KM.o[KM.o < 0] <- 0 #rounding error in diff
#Effective Sample Size
Ne.vec <- vector()
censor.type <- vector(length = nrow(dat))
censor.type[(dat[,2]) == 0] <- "R"
censor.type[(dat[,2]) == 1] <- "E"
for(i in 1:nrow(dat)){
if(censor.type[i] == "E") {
Ne.vec[i] <- 1
}
if(censor.type[i] == "R") {
Ne.vec[i] <- 1-(KM.s[match(dat[i,1], time)])
}
}
Ne <- sum(Ne.vec)
dat.tmp <- rbind(dat[,1], dat[,1])
dat.tmp <- rbind(dat.tmp, dat[,2])
dat.tmp[dat.tmp[,3] == 0, 2] <- NA
#set optimizer parameters
if(is.na(tolerance)){tolerance = time.int}
#global optimization
opts <- list("algorithm" = "NLOPT_GN_DIRECT_L",
"xtol_abs" = tolerance*500,
"maxeval" = 1000
)
#local optimization
opts2 <- list("algorithm" = "NLOPT_LN_BOBYQA",
"xtol_abs" = tolerance,
"maxeval" = 5000
)
#optimize
k.penal <- list("logN" = log(N), "logNm" = log(N*n.obs), "logNe" =  log(Ne))
bic.result <- list()
for(i in 1:length(penalty)){
kk <- as.numeric(k.penal[penalty][i])
bic <- nloptr::nloptr(0, bic.func, lb = 0, ub = (length(KM.o)*time.int), opts = opts,
dat = dat.tmp, uo = KM.o, time = time, inflection.threshold = inflection.threshold,
k = kk)
bic <- nloptr::nloptr(bic$solution, bic.func, lb = 0, ub = (length(KM.o)*time.int+time.int), opts = opts2,
dat = dat.tmp, uo = KM.o, time = time, inflection.threshold = inflection.threshold,
k = kk)
bic.result[[i]] <- bic
}
#number of censoring types
Ne <- sum(censor.type == "E")
Nr <- sum(censor.type == "R")
#write output
output <- list()
output[1:5] <- list(time,time[-1],N,Ne,Nr)
names(output)[1:5] <- c("time.s","time.o","N","Ne","Nr")
#number of iterations
for(i in 1:length(bic.result)){
output[[5+i]] <- bic.result[[i]]$iterations
names(output)[5+i] <- paste0(penalty[i],"_iterations")
}
#bic output
OL <- length(output)
for(i in 1:length(bic.result)){
output[[OL+i]] <- bic.result[[i]]$objective
names(output)[OL+i] <- paste0(penalty[i],"_bic")
}
#smoothed curves
OL <- length(output)
output[(OL+1):(OL+2)] <- list(KM.o,KM.s)
names(output)[(OL+1):(OL+2)] <- c("KM.o","KM.s")
OL <- length(output)
for(i in 1:length(bic.result)){
smooth.tmp=get.smooth.o(bic.result[[i]]$solution, KM.o, time)
output[[OL+2*i-1]] <- smooth.tmp
names(output)[OL+2*i-1] <- paste0("KM.o.smooth.",penalty[i])
output[[OL+2*i]] <- get.surv(smooth.tmp)
names(output)[OL+2*i] <- paste0("KM.s.smooth.",penalty[i])
}
#chosen bandwidth
OL <-length(output)
for(i in 1:length(bic.result)){
output[[OL+i]] <- bic.result[[i]]$solution
names(output)[OL+i] <- paste0(penalty[i],"_bw")
}
output[[length(output)+1]] <- dat
names(output)[length(output)] <- "dat"
return(output)
}
#' Internal function for calculating the sBIC
#'
bic.func <- function(z, dat, uo, time, inflection.threshold, k) {
if(z < (time[2]-time[1])) {uo.smooth <- uo
} else {
uo.smooth <- get.smooth.o(z, uo, time)
}
uo.smooth[is.na(uo.smooth)] <- 0
likelihood <- get.likelihood(dat, uo.smooth, time)
#changes in gradient
gc.tmp <- diff(uo.smooth)
#set below threshold to 0
gc.tmp[(abs(gc.tmp)) < inflection.threshold*mean(abs(gc.tmp), na.rm = T) & (abs(gc.tmp)) != 0] <- NA
gc.tmp <- zoo::na.locf(gc.tmp)
#inflection points
ip.tmp <- sign(gc.tmp)
ip <- sum(diff(ip.tmp) != 0) + 1 #for slope
#adds one if the curve doesn't start at 0
if(uo.smooth[1] != 0) ip <- ip+1
#loglik
bic <- -2*(sum(log(likelihood), na.rm = T)) + k*(ip)
return(bic)
}
#' Internal function for calculating the data likelihood given an assumed time-to-event density. Used by bic.func().
#'
get.likelihood <- function(dat, uo, time) {
#set empty storage vectors
likelihood <- vector()
dat[is.na(dat[,1]),1] <- 0
#get numerator and denominator contributions
for (i in 1:nrow(dat)) {
if(!is.na(dat[i,2])) {
#likelihood
likelihood[i] <- caTools::sumexact(uo[(match(dat[i,1],time)):(match(dat[i,2],time)-1)], na.rm = T)
} else {
likelihood[i] <- 1-(caTools::cumsumexact(uo)[match(dat[i,1], time)-1])
}
}
return(likelihood)
}
#' Internal function for counting the number of turning points in a time-to-event curve. Used by bic.func().
#'
get.tp <- function(uo.smooth, inflection.threshold) {
#changes in gradient
gc.tmp <- diff(uo.smooth)
#set below threshold to 0
gc.tmp[(abs(gc.tmp)) < inflection.threshold*mean(abs(gc.tmp), na.rm = T) & (abs(gc.tmp)) != 0] <- NA
gc.tmp <- na.locf(gc.tmp)
#inflection points
ip.tmp <- sign(gc.tmp)
ip <- sum(diff(ip.tmp) != 0) + 1 #for slope
#adds one if the curve doesn't start at 0
if(uo.smooth[1] != 0) ip <- ip+1
return(ip)
}
#' Internal function for converting a time-to-event distribution to a survival distribution.
#'
get.surv <- function(uo) {
surv.u <- vector()
surv.u[1] <- 1
for(i in 2:(length(uo)+1)) {
surv.u[i] <- surv.u[i-1] - uo[i-1]
}
return(surv.u)
}
#' Internal function for applying the Nadarayaâ€“Watson kernel regression smoother to a time-to-event distribution.
#'
get.smooth.o <- function(z, turnbull.o, time) {
uo <- turnbull.o
if(z < (time[2]-time[1])) {
uo.smooth <- turnbull.o
} else {
uo.smooth <- stats::ksmooth(x = time[-1], y = turnbull.o, kernel = "normal", bandwidth = z, n.points = length(turnbull.o))
uo.smooth <- uo.smooth$y
}
if(sum(uo.smooth, na.rm = T) > 1) {
uo.smooth <- uo.smooth/sum(uo.smooth, na.rm = T)
}
return(uo.smooth)
}
#' Imputation of Event Time
#'
#' Given a data frame or matrix of subject event intervals, this function imputes an estimate
#' of the exact onset time as the expectation given an assumed time-to-event distribution.
#'
#' The output is a vector of predicted event times for the appropriate subjects.
#'
#' @param dat A data.frame or matrix where rows are subjects and columns are left and right interval bounds.
#' @param event.distribution A vector of event probabilities, e.g. the TB.o.smooth.logNe vector output from
#' running the smoothTB function.
#' @param time A vector of event times corresponding to the probabilities in the event.distribution variable,
#' e.g. the time.o vector output from running the smoothTB function.
#' @param type The type of observation(s) you would like to predict. Mutually exclusive options are "LI" (left-
#' and interval-censored observations), "I" (interval-censored observations), "LIR" (left-, interval-, and right-
#' censored observations). Default is "I". Note that depending on the underlying event mechanism, it may be
#' inappropriate to attempt imputation for left- or right-censored subjects.
#'
#' @param n.dec The number of decimal places in the observed data.
#'
#' @export
predict.onset <- function(dat, event.distribution, time, type = "I", n.dec = 2) {
out.pred <- vector()
for(i in 1:nrow(dat)) {
dat.i <- as.numeric(unname(dat[i,]))
if(type == "LI") {
if(is.na(dat.i[2])) {
pred <- NA
} else {
if(is.na(dat.i[1])) dat.i[1] <- 1
times <- round(seq(dat.i[1],dat.i[2], by = (1/(10^(n.dec)))), n.dec)
times.p <- (match(times, time))
event.distribution[event.distribution == 0] <- .Machine$double.eps
pred <- sum(times*event.distribution[times.p]/sum(event.distribution[times.p], na.rm = T), na.rm = T)
}
}
if(type == "LIR") {
if(is.na(dat.i[2])) {
times <- round(seq(dat.i[1],max(time), by = (1/(10^(n.dec)))), n.dec)
times.p <- (match(times, time))
event.distribution[event.distribution == 0] <- .Machine$double.eps
pred <- sum(times*event.distribution[times.p]/sum(event.distribution[times.p], na.rm = T), na.rm = T)
} else {
if(is.na(dat.i[1])) dat.i[1] <- 1
times <- round(seq(dat.i[1],dat.i[2], by = (1/(10^(n.dec)))), n.dec)
times.p <- (match(times, time))
event.distribution[event.distribution == 0] <- .Machine$double.eps
pred <- sum(times*event.distribution[times.p]/sum(event.distribution[times.p], na.rm = T), na.rm = T)
}
}
if(type == "I") {
if(is.na(dat.i[2]) | is.na(dat.i[1])) {
pred <- NA
} else {
times <- round(seq(dat.i[1],dat.i[2], by = (1/(10^(n.dec)))), n.dec)
times.p <- (match(times, time))
event.distribution[event.distribution == 0] <- .Machine$double.eps
pred <- sum(times*event.distribution[times.p]/sum(event.distribution[times.p], na.rm = T), na.rm = T)
}
}
out.pred[i] <- pred
}
return(out.pred)
}
devtools::document()
setwd("~/Desktop/TMP/onset-estimation")
suppressMessages(source("simulate.onsets3.R"))
j = 17
s = 10
m0=c(30,50,70)
p0=c(0.1,1)
N0=c(100,5000)
n.obs0=c(2,6)
param=expand.grid(m0,p0,N0,n.obs0)
set.seed(22)
#generate data
dat <- simulate.onsets3(m = param[j,1], s = s, prev = param[j,2], N = param[j,3], n.obs = param[j,4])
dat.draws <- 1
#make sure there is at least 1 interval onset
while(sum(!is.na(dat[,1]) & !is.na(dat[,2])) < 1) {
dat <- simulate.onsets3(m = param[j,1], s = s, prev = param[j,2], N = param[j,3], n.obs = param[j,4])
dat.draws <- dat.draws + 1
}
dat2 <- dat[,1:2]
dat3 <- dat[,c(4,3)]
out3 <- smoothKM(dat3,
n.obs = 6,
penalty=c('logNm','logNe'))
out <- smoothTB(dat2,
n.obs = 6,
penalty=c('logN','logNe'))
use_readme_md(open = rlang::is_interactive())
library(usethis)
use_readme_md(open = rlang::is_interactive())
use_readme_rmd(open = rlang::is_interactive())
setwd("~/Documents/HKU/smooth_onset_package/SISE")
use_readme_rmd(open = rlang::is_interactive())
use_readme_rmd(open = rlang::is_interactive())
use_readme_rmd()
devtools::document()
#making package for smoothed survival analysis
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
#https://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html
library(devtools)
library(roxygen2)
library(usethis)
devtools::document()
#making package for smoothed survival analysis
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
#https://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html
library(devtools)
library(roxygen2)
library(usethis)
#create("SISE") #Smooth Interval-based Survival Estimation
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
use_readme_rmd()
library(SISE)
#A quick example using the breast cosmesis data from the interval package
data(bcos, package = "interval")
#Get the data into the correct format
dat <- bcos[,1:2]
dat$left[dat$left == 0] <- NA
dat$right[is.infinite(dat$right)] <- NA
#Estimate the model using the logNe penalty for selecting a smoothing bandwidth.
results <- smoothTB(dat = dat,
n.obs = 5,
left.bound = 0,
penalty=c('logNe'),
n.dec = 0)
#The results can be plotted by extracting the survival estimates from the output.
library(ggplot2)
plot.df <- data.frame(Time = results$time.s,
Survival = results$TB.s.smooth.logNe)
ggplot(plot.df, aes(x = Time, y = Survival)) +
geom_step()
p1 <- ggplot(plot.df.tb, aes(x = Time, y = Survival)) +
geom_step() + labs(title = "Smooth Turnbull Estimate")
plot.df <- data.frame(Time = results$time.s,
Survival = results$TB.s.smooth.logNe)
plot.df.tb <- data.frame(Time = results$time.s,
Survival = results$TB.s)
p1 <- ggplot(plot.df, aes(x = Time, y = Survival)) +
geom_step() + labs(title = "Smooth Turnbull Estimate")
p2 <- ggplot(plot.df.tb, aes(x = Time, y = Survival)) +
geom_step() + labs(title = "Turnbull Estimate")
gridExtra::grid.arrange(p1,p2)
?grid.arrange
gridExtra::grid.arrange(p1,p2, nrow=1)
results1 <- smoothTB(dat = dat[bcos$treatment == "Rad",],
n.obs = 5,
left.bound = 0,
penalty=c('logNe'),
n.dec = 0)
results2 <- smoothTB(dat = dat[bcos$treatment == "RadChem",],
n.obs = 5,
left.bound = 0,
penalty=c('logNe'),
n.dec = 0)
#The results can be plotted and compared to the original Turnbull by extracting the survival estimates from the output.
library(ggplot2)
plt.df <- data.frame(Time = c(results1$time.s, results2$time.s),
Survival = c(results1$TB.s.smooth.logNe,
results2$TB.s.smooth.logNe),
Treatment = factor(c(rep("Rad", length(results1$TB.s.smooth.logNe)),
rep("RadChem", length(results2$TB.s.smooth.logNe)))))
p1 <- ggplot(plt.df, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = " Smoothed Turnbull Estimate", x = "Time (months)")
plt.df.tb <- data.frame(Time = c(results1$time.s, results2$time.s),
Survival = c(results1$TB.s, results2$TB.s),
Treatment = factor(c(rep("Rad", length(results1$TB.s)),
rep("RadChem", length(results2$TB.s)))))
p2 <- ggplot(plt.df.tb, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = "Turnbull Estimate", x = "Time (months)")
gridExtra::grid.arrange(p1,p2)
gridExtra::grid.arrange(p1,p2, nrow = 1)
p1 <- ggplot(plt.df, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = " Smoothed Turnbull Estimate", x = "Time (months)") + theme(legend.position = "bottom")
p2 <- ggplot(plt.df.tb, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = "Turnbull Estimate", x = "Time (months)") + theme(legend.position = "bottom")
gridExtra::grid.arrange(p1,p2, nrow = 1)
#Estimate the model in the two treatment groups using the logNe penalty for selecting a smoothing bandwidth.
results1 <- smoothTB(dat = dat[bcos$treatment == "Rad",],
n.obs = 3,
left.bound = 0,
penalty=c('logNe'),
n.dec = 0)
results2 <- smoothTB(dat = dat[bcos$treatment == "RadChem",],
n.obs = 3,
left.bound = 0,
penalty=c('logNe'),
n.dec = 0)
#The results can be plotted and compared to the original Turnbull by extracting the survival estimates from the output.
library(ggplot2)
plt.df <- data.frame(Time = c(results1$time.s, results2$time.s),
Survival = c(results1$TB.s.smooth.logNe,
results2$TB.s.smooth.logNe),
Treatment = factor(c(rep("Rad", length(results1$TB.s.smooth.logNe)),
rep("RadChem", length(results2$TB.s.smooth.logNe)))))
p1 <- ggplot(plt.df, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = " Smoothed Turnbull Estimate", x = "Time (months)") + theme(legend.position = "bottom")
plt.df.tb <- data.frame(Time = c(results1$time.s, results2$time.s),
Survival = c(results1$TB.s, results2$TB.s),
Treatment = factor(c(rep("Rad", length(results1$TB.s)),
rep("RadChem", length(results2$TB.s)))))
p2 <- ggplot(plt.df.tb, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = "Turnbull Estimate", x = "Time (months)") + theme(legend.position = "bottom")
gridExtra::grid.arrange(p1,p2, nrow = 1)
#Estimate the model in the two treatment groups using the logNe penalty for selecting a smoothing bandwidth.
results1 <- smoothTB(dat = dat[bcos$treatment == "Rad",],
n.obs = 5,
left.bound = 0,
penalty=c('logNe'),
n.dec = 0)
results2 <- smoothTB(dat = dat[bcos$treatment == "RadChem",],
n.obs = 5,
left.bound = 0,
penalty=c('logNe'),
n.dec = 0)
#The results can be plotted and compared to the original Turnbull by extracting the survival estimates from the output.
library(ggplot2)
plt.df <- data.frame(Time = c(results1$time.s, results2$time.s),
Survival = c(results1$TB.s.smooth.logNe,
results2$TB.s.smooth.logNe),
Treatment = factor(c(rep("Rad", length(results1$TB.s.smooth.logNe)),
rep("RadChem", length(results2$TB.s.smooth.logNe)))))
p1 <- ggplot(plt.df, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = " Smoothed Turnbull Estimate", x = "Time (months)") + theme(legend.position = "bottom")
plt.df.tb <- data.frame(Time = c(results1$time.s, results2$time.s),
Survival = c(results1$TB.s, results2$TB.s),
Treatment = factor(c(rep("Rad", length(results1$TB.s)),
rep("RadChem", length(results2$TB.s)))))
p2 <- ggplot(plt.df.tb, aes(x = Time, y = Survival, color = Treatment)) +
geom_step() + theme_bw() + labs(title = "Turnbull Estimate", x = "Time (months)") + theme(legend.position = "bottom")
gridExtra::grid.arrange(p1,p2, nrow = 1)
library(SISE)
install("SISE")
setwd("~/Documents/HKU/smooth_onset_package/")
install("SISE")
devtools::document()
#create("SISE") #Smooth Interval-based Survival Estimation
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
setwd("~/Documents/HKU/smooth_onset_package/")
install("SISE")
devtools::document()
#create("SISE") #Smooth Interval-based Survival Estimation
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
setwd("~/Documents/HKU/smooth_onset_package/")
install("SISE")
#create("SISE") #Smooth Interval-based Survival Estimation
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
install.packages("devtools")
devtools::install_github("klutometis/roxygen")
#making package for smoothed survival analysis
#https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
#https://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html
library(devtools)
library(roxygen2)
library(usethis)
#create("SISE") #Smooth Interval-based Survival Estimation
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
setwd("~/Documents/HKU/smooth_onset_package/")
devtools::document()
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
setwd("~/Documents/HKU/smooth_onset_package/SISE/")
devtools::document()
devtools::document()
devtools::document()
setwd("~/Documents/HKU/smooth_onset_package/")
install("SISE")
```{r, fig.width=5, fig.retina = 2}
setwd("~/Documents/HKU/smooth_onset_package/SISE")
setwd("~/Documents/HKU/smooth_onset_package/SISE/vignettes")
setwd("~/Documents/HKU/smooth_onset_package/SISE")
setwd("~/Documents/HKU/smooth_onset_package/SISE/vignettes")
